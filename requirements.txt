# This file was autogenerated by uv via the following command:
#    uv export --no-hashes
aiohappyeyeballs==2.6.1
    # via
    #   aiohttp
    #   multi-llm-chat
aiohttp==3.13.3
    # via multi-llm-chat
aiosignal==1.4.0
    # via
    #   aiohttp
    #   multi-llm-chat
altair==5.5.0
    # via
    #   multi-llm-chat
    #   streamlit
annotated-types==0.7.0
    # via
    #   multi-llm-chat
    #   pydantic
anthropic==0.78.0
    # via multi-llm-chat
anyio==4.12.1
    # via
    #   anthropic
    #   google-genai
    #   httpx
    #   multi-llm-chat
    #   openai
astroid==3.3.9
    # via
    #   multi-llm-chat
    #   pylint
attrs==24.3.0
    # via
    #   aiohttp
    #   jsonschema
    #   multi-llm-chat
    #   referencing
beautifulsoup4==4.12.3
    # via
    #   google
    #   multi-llm-chat
black==25.1.0
    # via multi-llm-chat
blinker==1.9.0
    # via
    #   multi-llm-chat
    #   streamlit
blis==1.2.0
    # via
    #   multi-llm-chat
    #   thinc
cachetools==5.5.0
    # via
    #   google-auth
    #   multi-llm-chat
    #   streamlit
catalogue==2.0.10
    # via
    #   multi-llm-chat
    #   spacy
    #   srsly
    #   thinc
certifi==2026.1.4
    # via
    #   httpcore
    #   httpx
    #   multi-llm-chat
    #   requests
charset-normalizer==3.4.1
    # via
    #   multi-llm-chat
    #   requests
click==8.1.8
    # via
    #   black
    #   multi-llm-chat
    #   nltk
    #   streamlit
    #   typer
cloudpathlib==0.21.0
    # via
    #   multi-llm-chat
    #   weasel
colorama==0.4.6 ; sys_platform == 'win32'
    # via
    #   click
    #   multi-llm-chat
    #   pylint
    #   pytest
    #   tqdm
    #   wasabi
confection==0.1.5
    # via
    #   multi-llm-chat
    #   thinc
    #   weasel
coverage==7.7.1
    # via
    #   multi-llm-chat
    #   pytest-cov
curated-tokenizers==0.0.9
    # via
    #   multi-llm-chat
    #   spacy-curated-transformers
curated-transformers==0.1.1
    # via
    #   multi-llm-chat
    #   spacy-curated-transformers
cymem==2.0.11
    # via
    #   multi-llm-chat
    #   preshed
    #   spacy
    #   thinc
dill==0.3.9
    # via
    #   multi-llm-chat
    #   nlp
    #   pylint
distro==1.9.0
    # via
    #   anthropic
    #   multi-llm-chat
    #   openai
docker==7.1.0
    # via multi-llm-chat
docstring-parser==0.17.0
    # via
    #   anthropic
    #   multi-llm-chat
    #   phidata
ffmpeg==1.4
    # via multi-llm-chat
ffmpeg-python==0.2.0
    # via multi-llm-chat
filelock==3.20.3
    # via
    #   huggingface-hub
    #   multi-llm-chat
    #   nlp
    #   torch
    #   transformers
flash-attention==1.0.0
    # via multi-llm-chat
frozenlist==1.5.0
    # via
    #   aiohttp
    #   aiosignal
    #   multi-llm-chat
fsspec==2026.2.0
    # via
    #   huggingface-hub
    #   multi-llm-chat
    #   torch
future==1.0.0
    # via ffmpeg-python
gitdb==4.0.12
    # via
    #   gitpython
    #   multi-llm-chat
gitpython==3.1.44
    # via
    #   multi-llm-chat
    #   phidata
    #   streamlit
google==3.0.0
    # via multi-llm-chat
google-auth==2.37.0
    # via
    #   google-genai
    #   multi-llm-chat
google-genai==1.41.0
    # via multi-llm-chat
h11==0.16.0
    # via
    #   httpcore
    #   multi-llm-chat
hf-xet==1.2.0 ; platform_machine == 'aarch64' or platform_machine == 'amd64' or platform_machine == 'arm64' or platform_machine == 'x86_64'
    # via huggingface-hub
httpcore==1.0.9
    # via
    #   httpx
    #   multi-llm-chat
httpx==0.28.1
    # via
    #   anthropic
    #   google-genai
    #   multi-llm-chat
    #   ollama
    #   openai
    #   phidata
huggingface-hub==0.36.2
    # via
    #   multi-llm-chat
    #   tokenizers
    #   transformers
idna==3.11
    # via
    #   anyio
    #   httpx
    #   multi-llm-chat
    #   requests
    #   yarl
iniconfig==2.1.0
    # via
    #   multi-llm-chat
    #   pytest
isort==6.0.1
    # via
    #   multi-llm-chat
    #   pylint
jinja2==3.1.6
    # via
    #   altair
    #   mlx-lm
    #   multi-llm-chat
    #   pydeck
    #   spacy
    #   torch
jiter==0.13.0
    # via
    #   anthropic
    #   multi-llm-chat
    #   openai
joblib==1.4.2
    # via
    #   multi-llm-chat
    #   nltk
    #   scikit-learn
jsonschema==4.23.0
    # via
    #   altair
    #   multi-llm-chat
jsonschema-specifications==2024.10.1
    # via
    #   jsonschema
    #   multi-llm-chat
langcodes==3.5.0
    # via
    #   multi-llm-chat
    #   spacy
language-data==1.3.0
    # via
    #   langcodes
    #   multi-llm-chat
marisa-trie==1.2.1
    # via
    #   language-data
    #   multi-llm-chat
markdown-it-py==3.0.0
    # via
    #   multi-llm-chat
    #   rich
markupsafe==3.0.3
    # via
    #   jinja2
    #   multi-llm-chat
mccabe==0.7.0
    # via
    #   multi-llm-chat
    #   pylint
mdurl==0.1.2
    # via
    #   markdown-it-py
    #   multi-llm-chat
mlx==0.30.6
    # via
    #   mlx-lm
    #   multi-llm-chat
mlx-lm==0.29.1
    # via multi-llm-chat
mlx-metal==0.30.6
    # via
    #   mlx
    #   multi-llm-chat
mpmath==1.3.0
    # via
    #   multi-llm-chat
    #   sympy
multidict==6.1.0
    # via
    #   aiohttp
    #   multi-llm-chat
    #   yarl
murmurhash==1.0.12
    # via
    #   multi-llm-chat
    #   preshed
    #   spacy
    #   thinc
mypy-extensions==1.0.0
    # via
    #   black
    #   multi-llm-chat
narwhals==1.23.0
    # via
    #   altair
    #   multi-llm-chat
    #   plotly
networkx==3.6.1
    # via
    #   multi-llm-chat
    #   torch
nlp==0.4.0
    # via multi-llm-chat
nltk==3.9.1
    # via
    #   multi-llm-chat
    #   textblob
numpy==2.2.2
    # via
    #   blis
    #   mlx-lm
    #   multi-llm-chat
    #   nlp
    #   opencv-python
    #   pandas
    #   pydeck
    #   scikit-learn
    #   scipy
    #   spacy
    #   streamlit
    #   thinc
    #   transformers
nvidia-cublas-cu12==12.8.4.1 ; platform_machine == 'x86_64' and sys_platform == 'linux'
    # via
    #   nvidia-cudnn-cu12
    #   nvidia-cusolver-cu12
    #   torch
nvidia-cuda-cupti-cu12==12.8.90 ; platform_machine == 'x86_64' and sys_platform == 'linux'
    # via torch
nvidia-cuda-nvrtc-cu12==12.8.93 ; platform_machine == 'x86_64' and sys_platform == 'linux'
    # via torch
nvidia-cuda-runtime-cu12==12.8.90 ; platform_machine == 'x86_64' and sys_platform == 'linux'
    # via torch
nvidia-cudnn-cu12==9.10.2.21 ; platform_machine == 'x86_64' and sys_platform == 'linux'
    # via torch
nvidia-cufft-cu12==11.3.3.83 ; platform_machine == 'x86_64' and sys_platform == 'linux'
    # via torch
nvidia-cufile-cu12==1.13.1.3 ; platform_machine == 'x86_64' and sys_platform == 'linux'
    # via torch
nvidia-curand-cu12==10.3.9.90 ; platform_machine == 'x86_64' and sys_platform == 'linux'
    # via torch
nvidia-cusolver-cu12==11.7.3.90 ; platform_machine == 'x86_64' and sys_platform == 'linux'
    # via torch
nvidia-cusparse-cu12==12.5.8.93 ; platform_machine == 'x86_64' and sys_platform == 'linux'
    # via
    #   nvidia-cusolver-cu12
    #   torch
nvidia-cusparselt-cu12==0.7.1 ; platform_machine == 'x86_64' and sys_platform == 'linux'
    # via torch
nvidia-nccl-cu12==2.27.5 ; platform_machine == 'x86_64' and sys_platform == 'linux'
    # via torch
nvidia-nvjitlink-cu12==12.8.93 ; platform_machine == 'x86_64' and sys_platform == 'linux'
    # via
    #   nvidia-cufft-cu12
    #   nvidia-cusolver-cu12
    #   nvidia-cusparse-cu12
    #   torch
nvidia-nvshmem-cu12==3.3.20 ; platform_machine == 'x86_64' and sys_platform == 'linux'
    # via torch
nvidia-nvtx-cu12==12.8.90 ; platform_machine == 'x86_64' and sys_platform == 'linux'
    # via torch
ollama==0.6.1
    # via multi-llm-chat
openai==2.17.0
    # via multi-llm-chat
opencv-python==4.12.0.88
    # via multi-llm-chat
packaging==24.2
    # via
    #   altair
    #   black
    #   huggingface-hub
    #   multi-llm-chat
    #   plotly
    #   pytest
    #   spacy
    #   streamlit
    #   thinc
    #   transformers
    #   weasel
pandas==2.2.3
    # via
    #   multi-llm-chat
    #   nlp
    #   streamlit
pathspec==0.12.1
    # via
    #   black
    #   multi-llm-chat
phidata==2.7.10
    # via multi-llm-chat
pillow==11.1.0
    # via
    #   multi-llm-chat
    #   streamlit
platformdirs==4.3.7
    # via
    #   black
    #   multi-llm-chat
    #   pylint
plotly==6.0.1
    # via multi-llm-chat
pluggy==1.5.0
    # via
    #   multi-llm-chat
    #   pytest
preshed==3.0.9
    # via
    #   multi-llm-chat
    #   spacy
    #   thinc
propcache==0.2.1
    # via
    #   aiohttp
    #   multi-llm-chat
    #   yarl
protobuf==6.33.5
    # via
    #   mlx-lm
    #   streamlit
psutil==7.0.0
    # via multi-llm-chat
pyarrow==19.0.0
    # via
    #   multi-llm-chat
    #   nlp
    #   streamlit
pyasn1==0.6.2
    # via
    #   multi-llm-chat
    #   pyasn1-modules
    #   rsa
pyasn1-modules==0.4.1
    # via
    #   google-auth
    #   multi-llm-chat
pydantic==2.12.5
    # via
    #   anthropic
    #   confection
    #   google-genai
    #   multi-llm-chat
    #   ollama
    #   openai
    #   phidata
    #   pydantic-settings
    #   spacy
    #   thinc
    #   weasel
pydantic-core==2.41.5
    # via
    #   multi-llm-chat
    #   pydantic
pydantic-settings==2.8.1
    # via
    #   multi-llm-chat
    #   phidata
pydeck==0.9.1
    # via
    #   multi-llm-chat
    #   streamlit
pygments==2.19.1
    # via
    #   multi-llm-chat
    #   rich
pylint==3.3.6
    # via multi-llm-chat
pytest==8.0.0
    # via
    #   multi-llm-chat
    #   pytest-cov
pytest-cov==4.1.0
    # via multi-llm-chat
python-dateutil==2.9.0.post0
    # via
    #   multi-llm-chat
    #   pandas
python-dotenv==1.0.1
    # via
    #   multi-llm-chat
    #   phidata
    #   pydantic-settings
pytz==2024.2
    # via
    #   multi-llm-chat
    #   pandas
pywin32==311 ; sys_platform == 'win32'
    # via docker
pyyaml==6.0.2
    # via
    #   huggingface-hub
    #   mlx-lm
    #   multi-llm-chat
    #   phidata
    #   transformers
referencing==0.36.1
    # via
    #   jsonschema
    #   jsonschema-specifications
    #   multi-llm-chat
regex==2024.11.6
    # via
    #   curated-tokenizers
    #   multi-llm-chat
    #   nltk
    #   transformers
requests==2.32.4
    # via
    #   docker
    #   google-genai
    #   huggingface-hub
    #   multi-llm-chat
    #   nlp
    #   spacy
    #   streamlit
    #   transformers
    #   weasel
rich==13.9.4
    # via
    #   multi-llm-chat
    #   phidata
    #   streamlit
    #   typer
rpds-py==0.22.3
    # via
    #   jsonschema
    #   multi-llm-chat
    #   referencing
rsa==4.9
    # via
    #   google-auth
    #   multi-llm-chat
safetensors==0.5.2
    # via
    #   multi-llm-chat
    #   transformers
scikit-learn==1.7.2
    # via multi-llm-chat
scipy==1.15.2
    # via
    #   multi-llm-chat
    #   scikit-learn
sentencepiece==0.2.1
    # via
    #   mlx-lm
    #   multi-llm-chat
setuptools==81.0.0
    # via
    #   marisa-trie
    #   multi-llm-chat
    #   spacy
    #   thinc
    #   torch
shellingham==1.5.4
    # via
    #   multi-llm-chat
    #   typer
six==1.17.0
    # via
    #   multi-llm-chat
    #   python-dateutil
smart-open==7.1.0
    # via
    #   multi-llm-chat
    #   weasel
smmap==5.0.2
    # via
    #   gitdb
    #   multi-llm-chat
sniffio==1.3.1
    # via
    #   anthropic
    #   multi-llm-chat
    #   openai
soupsieve==2.6
    # via
    #   beautifulsoup4
    #   multi-llm-chat
spacy==3.8.4
    # via
    #   multi-llm-chat
    #   spacytextblob
spacy-curated-transformers==0.3.0
    # via multi-llm-chat
spacy-legacy==3.0.12
    # via
    #   multi-llm-chat
    #   spacy
spacy-loggers==1.0.5
    # via
    #   multi-llm-chat
    #   spacy
spacytextblob==5.0.0
    # via multi-llm-chat
srsly==2.5.1
    # via
    #   confection
    #   multi-llm-chat
    #   spacy
    #   thinc
    #   weasel
streamlit==1.41.1
    # via multi-llm-chat
sympy==1.14.0
    # via
    #   multi-llm-chat
    #   torch
tenacity==9.1.3
    # via
    #   google-genai
    #   streamlit
textblob==0.19.0
    # via
    #   multi-llm-chat
    #   spacytextblob
thinc==8.3.4
    # via
    #   multi-llm-chat
    #   spacy
threadpoolctl==3.6.0
    # via
    #   multi-llm-chat
    #   scikit-learn
tokenizers==0.21.0
    # via
    #   multi-llm-chat
    #   transformers
toml==0.10.2
    # via
    #   multi-llm-chat
    #   streamlit
tomli==2.2.1
    # via
    #   multi-llm-chat
    #   phidata
tomlkit==0.13.2
    # via
    #   multi-llm-chat
    #   pylint
torch==2.10.0
    # via
    #   curated-transformers
    #   multi-llm-chat
    #   spacy-curated-transformers
tornado==6.5.4
    # via streamlit
tqdm==4.67.1
    # via
    #   huggingface-hub
    #   multi-llm-chat
    #   nlp
    #   nltk
    #   openai
    #   spacy
    #   transformers
transformers==4.53.0
    # via
    #   mlx-lm
    #   multi-llm-chat
triton==3.5.0 ; platform_machine == 'x86_64' and sys_platform == 'linux'
    # via torch
typer==0.15.2
    # via
    #   multi-llm-chat
    #   phidata
    #   spacy
    #   weasel
typing-extensions==4.15.0
    # via
    #   aiosignal
    #   altair
    #   anthropic
    #   anyio
    #   google-genai
    #   huggingface-hub
    #   multi-llm-chat
    #   openai
    #   phidata
    #   pydantic
    #   pydantic-core
    #   referencing
    #   streamlit
    #   torch
    #   typer
    #   typing-inspection
typing-inspection==0.4.2
    # via pydantic
tzdata==2025.1
    # via
    #   multi-llm-chat
    #   pandas
urllib3==2.6.3
    # via
    #   docker
    #   multi-llm-chat
    #   requests
wasabi==1.1.3
    # via
    #   multi-llm-chat
    #   spacy
    #   thinc
    #   weasel
watchdog==6.0.0
    # via
    #   multi-llm-chat
    #   streamlit
weasel==0.4.1
    # via
    #   multi-llm-chat
    #   spacy
websockets==14.2
    # via
    #   google-genai
    #   multi-llm-chat
wrapt==1.17.2
    # via
    #   multi-llm-chat
    #   smart-open
xxhash==3.5.0
    # via
    #   multi-llm-chat
    #   nlp
yarl==1.18.3
    # via
    #   aiohttp
    #   multi-llm-chat
